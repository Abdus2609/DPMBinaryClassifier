{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d387a9ae-3618-4615-bc29-0540fa535ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (4.38.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers) (0.21.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: requests in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: filelock in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from requests->transformers) (2.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from requests->transformers) (3.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5f13176-f951-44cd-a0a9-f8fd659a6bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2203463/1267011614.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "\n",
    "file_path = \"train_data/dontpatronizeme_pcl.tsv\"\n",
    "train_filepath = \"dev_data/train_semeval_parids-labels.csv\"\n",
    "dev_filepath = \"dev_data/dev_semeval_parids-labels.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, sep='\\t', header=2, names=['id', 'paragraph-id', 'keyword', 'countrycode', \"paragraph\", \"label\"])\n",
    "df_filtered = df[df['paragraph'].notna()]\n",
    "\n",
    "train_df = pd.read_csv(train_filepath)\n",
    "dev_df = pd.read_csv(dev_filepath)\n",
    "\n",
    "train_data = df_filtered[df_filtered['id'].isin(train_df['par_id'])]\n",
    "dev_data = df_filtered[df_filtered['id'].isin(dev_df['par_id'])]\n",
    "\n",
    "train_data_shuffled = shuffle(train_data, random_state=42)\n",
    "dev_data_shuffled = shuffle(dev_data, random_state=42)\n",
    "\n",
    "X_train = train_data_shuffled['paragraph'].to_numpy()\n",
    "X_dev = dev_data_shuffled['paragraph'].to_numpy()\n",
    "\n",
    "y_train = [0 if int(x) <= 1 else 1 for x in train_data_shuffled['label']]\n",
    "y_dev = [0 if int(x) <= 1 else 1 for x in dev_data_shuffled['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f834d797-6ac4-40fb-8bb5-b12f37bcebeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraph-id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>countrycode</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4822</th>\n",
       "      <td>4823</td>\n",
       "      <td>@@8781228</td>\n",
       "      <td>disabled</td>\n",
       "      <td>gb</td>\n",
       "      <td>As well as lying about helping flood victims ,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8324</th>\n",
       "      <td>8325</td>\n",
       "      <td>@@14942580</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>ng</td>\n",
       "      <td>Betty Abah is passionate about this initiative...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>2384</td>\n",
       "      <td>@@7600715</td>\n",
       "      <td>in-need</td>\n",
       "      <td>sg</td>\n",
       "      <td>\" He liked to help people so I thought this co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4288</th>\n",
       "      <td>4289</td>\n",
       "      <td>@@8869471</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>ng</td>\n",
       "      <td>\" The airlines are relatively small , weak and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5507</th>\n",
       "      <td>5508</td>\n",
       "      <td>@@23720891</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ie</td>\n",
       "      <td>In general , people live inside their own bubb...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5846</th>\n",
       "      <td>5847</td>\n",
       "      <td>@@19919480</td>\n",
       "      <td>homeless</td>\n",
       "      <td>my</td>\n",
       "      <td>Last year , a record 85 homes were demolished ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5290</th>\n",
       "      <td>5291</td>\n",
       "      <td>@@21695353</td>\n",
       "      <td>homeless</td>\n",
       "      <td>pk</td>\n",
       "      <td>\" As a country , we can look for the missed op...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5491</th>\n",
       "      <td>5492</td>\n",
       "      <td>@@14069020</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>hk</td>\n",
       "      <td>Any opening in which the speakers can revert t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>880</td>\n",
       "      <td>@@24188457</td>\n",
       "      <td>in-need</td>\n",
       "      <td>ke</td>\n",
       "      <td>Dennis insisted that his initiative was not in...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7421</th>\n",
       "      <td>7422</td>\n",
       "      <td>@@1431279</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>ca</td>\n",
       "      <td>Developing something like this for commercial ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8375 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id paragraph-id     keyword countrycode  \\\n",
       "4822  4823    @@8781228    disabled          gb   \n",
       "8324  8325   @@14942580  vulnerable          ng   \n",
       "2383  2384    @@7600715     in-need          sg   \n",
       "4288  4289    @@8869471  vulnerable          ng   \n",
       "5507  5508   @@23720891     refugee          ie   \n",
       "...    ...          ...         ...         ...   \n",
       "5846  5847   @@19919480    homeless          my   \n",
       "5290  5291   @@21695353    homeless          pk   \n",
       "5491  5492   @@14069020   immigrant          hk   \n",
       "879    880   @@24188457     in-need          ke   \n",
       "7421  7422    @@1431279    hopeless          ca   \n",
       "\n",
       "                                              paragraph  label  \n",
       "4822  As well as lying about helping flood victims ,...      0  \n",
       "8324  Betty Abah is passionate about this initiative...      4  \n",
       "2383  \" He liked to help people so I thought this co...      1  \n",
       "4288  \" The airlines are relatively small , weak and...      0  \n",
       "5507  In general , people live inside their own bubb...      3  \n",
       "...                                                 ...    ...  \n",
       "5846  Last year , a record 85 homes were demolished ...      0  \n",
       "5290  \" As a country , we can look for the missed op...      0  \n",
       "5491  Any opening in which the speakers can revert t...      0  \n",
       "879   Dennis insisted that his initiative was not in...      3  \n",
       "7421  Developing something like this for commercial ...      0  \n",
       "\n",
       "[8375 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd52e609-3775-419e-9f5c-2eb7c8bef024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraph-id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>countrycode</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10056</th>\n",
       "      <td>10057</td>\n",
       "      <td>@@4197415</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>ca</td>\n",
       "      <td>Darte acknowledged cutting back to the Windsor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9650</th>\n",
       "      <td>9651</td>\n",
       "      <td>@@25216962</td>\n",
       "      <td>migrant</td>\n",
       "      <td>bd</td>\n",
       "      <td>UNITED States President Donald Trump has defen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9119</th>\n",
       "      <td>9120</td>\n",
       "      <td>@@22467955</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>ca</td>\n",
       "      <td>Saraswat said most immigrants have unique livi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8504</th>\n",
       "      <td>8505</td>\n",
       "      <td>@@10179731</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>pk</td>\n",
       "      <td>He said some elements were bent upon spreading...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>1283</td>\n",
       "      <td>@@3208839</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ph</td>\n",
       "      <td>\" Stateless \" is the story of a forgotten grou...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>9978</td>\n",
       "      <td>@@13589752</td>\n",
       "      <td>homeless</td>\n",
       "      <td>in</td>\n",
       "      <td>One response to marital infidelity is divorce ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9384</th>\n",
       "      <td>9385</td>\n",
       "      <td>@@1955909</td>\n",
       "      <td>homeless</td>\n",
       "      <td>tz</td>\n",
       "      <td>Various other areas have been experiencing exc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9423</th>\n",
       "      <td>9424</td>\n",
       "      <td>@@18374692</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>ca</td>\n",
       "      <td>Chris Selley : Maybe liquor retail in Ontario ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9594</th>\n",
       "      <td>9595</td>\n",
       "      <td>@@1065878</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>us</td>\n",
       "      <td>Robin Wauters is the European Editor of The Ne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9136</th>\n",
       "      <td>9137</td>\n",
       "      <td>@@52188</td>\n",
       "      <td>disabled</td>\n",
       "      <td>ie</td>\n",
       "      <td>\" It ranges from members of the Oireachtas , t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2093 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id paragraph-id        keyword countrycode  \\\n",
       "10056  10057    @@4197415  poor-families          ca   \n",
       "9650    9651   @@25216962        migrant          bd   \n",
       "9119    9120   @@22467955      immigrant          ca   \n",
       "8504    8505   @@10179731       hopeless          pk   \n",
       "1282    1283    @@3208839        refugee          ph   \n",
       "...      ...          ...            ...         ...   \n",
       "9977    9978   @@13589752       homeless          in   \n",
       "9384    9385    @@1955909       homeless          tz   \n",
       "9423    9424   @@18374692       hopeless          ca   \n",
       "9594    9595    @@1065878       hopeless          us   \n",
       "9136    9137      @@52188       disabled          ie   \n",
       "\n",
       "                                               paragraph  label  \n",
       "10056  Darte acknowledged cutting back to the Windsor...      0  \n",
       "9650   UNITED States President Donald Trump has defen...      0  \n",
       "9119   Saraswat said most immigrants have unique livi...      0  \n",
       "8504   He said some elements were bent upon spreading...      0  \n",
       "1282   \" Stateless \" is the story of a forgotten grou...      4  \n",
       "...                                                  ...    ...  \n",
       "9977   One response to marital infidelity is divorce ...      0  \n",
       "9384   Various other areas have been experiencing exc...      0  \n",
       "9423   Chris Selley : Maybe liquor retail in Ontario ...      0  \n",
       "9594   Robin Wauters is the European Editor of The Ne...      0  \n",
       "9136   \" It ranges from members of the Oireachtas , t...      0  \n",
       "\n",
       "[2093 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d5bee13-afff-41c7-8a85-6384b6640f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8375\n",
      "2093\n",
      "8375\n",
      "2093\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_dev))\n",
    "print(len(y_train))\n",
    "print(len(y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e374b79-8296-4d75-9a40-7755a4d9310d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: cugcnov1\n",
      "Sweep URL: https://wandb.ai/eli-carried/uncategorized/sweeps/cugcnov1\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'eval/f1_score',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "\n",
    "        'epochs': {\n",
    "            'values' : [3, 5, 7]\n",
    "        },\n",
    "\n",
    "        'batch_size': {\n",
    "            'values': [8, 16, 32]\n",
    "        },\n",
    "\n",
    "        'warmup_steps': {\n",
    "            'values': [100, 500]\n",
    "        },\n",
    "\n",
    "        'learning_rate': {\n",
    "            'values': [1e-5, 2e-5, 3e-5]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_defaults = {\n",
    "        'learning_rate': 2e-5,\n",
    "\n",
    "        'batch_size': 16,\n",
    "\n",
    "        'epochs': 5,\n",
    "\n",
    "        'warmup_steps': 500\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12be0a7e-f29e-42a9-a6e8-35938fbd3ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33b3e0c-51a5-44cd-9d3b-b7e4b5fbdb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aea042a-2041-4cd7-9911-752dfcfa0f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (0.27.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from accelerate) (2.2.0)\n",
      "Requirement already satisfied: huggingface-hub in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from accelerate) (0.21.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from accelerate) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: triton==2.2.0 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: fsspec in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.12.2)\n",
      "Requirement already satisfied: networkx in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: sympy in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: filelock in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: jinja2 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
      "Requirement already satisfied: requests in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: transformers[torch] in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (4.38.2)\n",
      "Requirement already satisfied: requests in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\n",
      "Requirement already satisfied: numpy>=1.17 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers[torch]) (1.26.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers[torch]) (0.15.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers[torch]) (0.21.3)\n",
      "Requirement already satisfied: filelock in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers[torch]) (0.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: torch in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers[torch]) (2.2.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from transformers[torch]) (0.27.2)\n",
      "Requirement already satisfied: psutil in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch->transformers[torch]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch->transformers[torch]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch->transformers[torch]) (11.4.5.107)\n",
      "Requirement already satisfied: jinja2 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch->transformers[torch]) (2.19.3)\n",
      "Requirement already satisfied: sympy in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch->transformers[torch]) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch->transformers[torch]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: networkx in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.3.101)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from requests->transformers[torch]) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /vol/bitbucket/aah120/myenv/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U\n",
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4505c6e7-0004-4b24-ac4e-e7cfc52e4a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Call the garbage collector\n",
    "gc.collect()\n",
    "    \n",
    "# Ensure CUDA is aware of the freed memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    \n",
    "    return {\"f1_score\": f1, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "def train():\n",
    "\n",
    "    wandb.init()\n",
    "\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    max_length = 300\n",
    "    dataset = CustomDataset(X_train, y_train, tokenizer, max_length)\n",
    "\n",
    "    train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "    model.to(device)\n",
    "    # train_dataset = torch.tensor(train_dataset).to(device)\n",
    "    # val_dataset = torch.tensor(val_dataset).to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          \n",
    "        per_device_train_batch_size=wandb.config.batch_size,  \n",
    "        per_device_eval_batch_size=wandb.config.batch_size,\n",
    "        num_train_epochs=wandb.config.epochs,\n",
    "        learning_rate=wandb.config.learning_rate,\n",
    "        warmup_steps=wandb.config.warmup_steps,\n",
    "        evaluation_strategy=\"epoch\",     \n",
    "        logging_dir='./logs',            \n",
    "        logging_steps=100,\n",
    "        metric_for_best_model = 'f1',\n",
    "        report_to=\"wandb\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    predictions = trainer.predict(val_dataset)\n",
    "    preds = predictions.predictions.argmax(-1)\n",
    "    labels = predictions.label_ids\n",
    "\n",
    "    f1 = f1_score(labels, preds)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "\n",
    "\n",
    "    wandb.log({\"f1_score\": f1,\n",
    "               \"accuracy\": accuracy})\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Call the garbage collector\n",
    "    gc.collect()\n",
    "    \n",
    "    # Ensure CUDA is aware of the freed memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9e3ef5-07b1-4f82-acc3-8c912b1ff934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11abe8e5-9c05-48d2-a8a8-a2f8c1f750c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Call the garbage collector\n",
    "gc.collect()\n",
    "    \n",
    "# Ensure CUDA is aware of the freed memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "840b210a-cc0b-478c-bbdc-85487865ca8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[{'input_ids': tensor([  101,  5920,  1999, 16634, 11431,  4904,  2003,  3303,  2011,  1037,\n",
      "         2193,  1997,  5876,  1010,  2164,  2740,  3471,  1010,  5850,  1998,\n",
      "         6544,  1010, 18917,  1010,  5635,  1998, 20625,  2791,  1010,  2016,\n",
      "         2056,  1010,  1998,  1000,  1999,  2344,  2000,  2644,  2111,  2013,\n",
      "        16873,  5920,  1010,  2057,  2342,  2000,  2298,  2012,  2122,  3314,\n",
      "         1012,  2065,  2057,  2079,  1050,  1005,  1056,  2156,  2068,  2030,\n",
      "         2963,  2055,  2009,  1010,  2009, 24185,  1050,  1005,  1056,  2175,\n",
      "         2185,  1012,  1000,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(0)}, {'input_ids': tensor([  101,  1996,  6829,  2001,  2170,  2011,  2037,  2120,  2374,  4657,\n",
      "         1006, 11865,  7011,  1007,  2000,  3693,  1996,  2717,  1997,  1996,\n",
      "         4686,  7732,  2039,  2005,  1996,  5379,  1012,  2174,  8675,  2050,\n",
      "         2024,  2036, 25478,  1999,  2342,  1997,  1996,  2867,  2005,  2037,\n",
      "        10232, 24689,  3966,  2223,  2709,  4190,  2674,  2114, 13304,  2632,\n",
      "        18347,  2100,  2000,  2022,  2209,  1999, 11096,  2006,  4465,  1012,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(0)}, {'input_ids': tensor([  101,  2028,  1997,  1996,  2599,  6048,  1997,  1996,  2040,  2817,\n",
      "         1010,  2934, 16686,  3593,  1041, 20715,  3775,  1997,  1996,  2082,\n",
      "         1997,  2270,  2740,  2012,  4461,  2267,  2414,  1010,  2758,  1024,\n",
      "         1000,  2122, 15366, 12878,  8339,  1996,  4254,  1997,  2833,  5821,\n",
      "         1998,  6043,  2408,  1996,  7595,  1010,  2007,  7965, 17490, 14778,\n",
      "         6313,  9440,  2205,  6450,  2005,  3532,  2945,  1998,  4279,  1012,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(0)}, {'input_ids': tensor([  101,  2348,  7269,  2772,  1996,  4895,  4680,  2006,  1996,  2916,\n",
      "         1997,  9776,  2111,  1010,  1996,  2231,  2145,  2515,  2025,  2031,\n",
      "        23411,  6747,  2006,  1996,  2193,  2030,  2110,  1997,  2107,  2111,\n",
      "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(0)}, {'input_ids': tensor([  101, 26062,  7164,  1057,  2497,  1005,  1055,  3891,  6337,  2038,\n",
      "         5301,  2206,  1037,  5670,  2875, 10940,  2000,  3469,  5971,  2015,\n",
      "         2013, 15488,  2229,  1010,  2029,  2024,  2062,  8211,  1999,  3171,\n",
      "         2091, 22299,  2015,  1010,  1998,  2488,  3891,  2968,  1012,  2023,\n",
      "         2071,  2490,  2488, 11412,  3737,  2084,  1999,  1996,  2627,  1012,\n",
      "         2045,  2038,  2042,  1037,  4629,  6689,  1999,  1057,  2497,  1005,\n",
      "         1055,  2988,  7977, 27937,  2140,  6463,  2000,  1018,  1012,  6584,\n",
      "         1003,  2012,  2203,  1011,  2238,  2325,  2013,  1022,  1012,  2423,\n",
      "         1003,  2012,  2203,  1011,  2297,  1012,  2023,  3275, 23329,  2015,\n",
      "        27937,  4877,  2012,  2049,  7506,  1057,  2497,  5446,  1010,  2029,\n",
      "         2815,  3278,  1998, 14729,  2005,  3943,  1012,  1019,  1003,  1997,\n",
      "         1996,  2967,  1005,  2561, 27937,  4877,  2012,  2203,  1011,  2238,\n",
      "         2325,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(0)}]\n"
     ]
    }
   ],
   "source": [
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    max_length = 300\n",
    "    dataset = CustomDataset(X_train, y_train, tokenizer, max_length)\n",
    "\n",
    "    train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "print(type(train_dataset))\n",
    "print(train_dataset[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
